7.26
发现Platt-SMO对setosa与versicolor进行二分类时表现良好（参数设置C=0.9; tol=0.001; maxIter=20; 正确率0.95，即有一个分类错误）
涉及virginica的二分类时测试集有多于一半错误（参数设置C=0.8; tol=0.001; maxIter=20; 正确率0.45）

三分类时：参数设置: C=0.0001; tol=0.1; maxIter=40；正确率0.87

7.27
引入高斯核函数之后，涉及virginica的二分类时测试准确度大幅提高
发现virginica与setosa的测试中kTup越小，测试准确率越高
（参数设置C=0.9; tol=0.001; maxIter=20; kTup=0.5;正确率0.95，即有一个分类错误）

三分类时
kTup=('rbf',2.6);C=0.8;tol=0.001;maxIter=40
kTup1=('rbf',1.3);C1=0.9;tol1=0.001;maxIter1=20
预测正确率大概为0.77(0.73~0.9，不知道为什么会出现0.9)
sklearn的测试正确率为0.867（C=2,gamma=10）
Platt-SMO具有随机性，sklearn的SVM不具有随机性

8.31
换了一台电脑，原来是Linux现在是windows，数据集路径变了，数据集格式变了，原代码不能跑了（???）。
重构了一下svm的代码，但是我不是很能理解我之前照书打的Platt-SMO.py为什么可以走
（我看不懂代码写的是什么，我怀疑我打错了很多地方，但是它确实可以走，???）
SVM.py是参照Platt给出的SMO伪代码写的，数据结构和部分逻辑参照周志华机器学习中给出py代码
发现对b更新不起作用，w是可以正确分类的，但是偏移量b很有问题
（我不知道为什么）
所以我把对b的更新删了（反正其实用不上），最后再求b

训练集为每一类前40个，测试集为每一类后10个

下面为分类器训练结果：

    不采用核函数时，3分类器正确率在0.6~1.0 之间浮动，分类模型习惯把versicolor与virginica分类错
    参数配置为：
        C = 0.8
        tol = 0.01
        maxIter = 20 或 30
        kTup = ['lin', 0.6]

    采用gauss核函数(kTup=0.6)时，20次测试中3分类器正确率在0.6~1.0 之间浮动

    0.6~0.7: 2 times;
    0.7~0.8: 4 times;
    0.8~0.9: 3 times;
    0.9~1.0: 10 times;
    1.0: 1 times;

    分类模型习惯把versicolor与virginica分类错
    参数:
        C = 0.8
        tol = 0.01
        maxIter = 30
        kTup = ['rbf', 0.6]

    采用gauss核函数(kTup=0.5)时，20次测试中3分类器正确率在0.6~1.0 之间浮动

    0.6~0.7: 1 times;
    0.7~0.8: 3 times;
    0.8~0.9: 1 times;
    0.9~1.0: 10 times;
    1.0: 5  times;

    忘记看分类模型的习惯错误了
    参数:
        C = 0.8
        tol = 0.01
        maxIter = 30
        kTup = ['rbf', 0.5]

9.8
减少训练集和测试集的规模
60k太大了...减少到600
先把参数测出来。

9.10
训练集减少到 600，测试集减少到 200,只涉及 0 和 1 的分类使用线性分类器
测了4次正确率全是 1 不测了

线性分类器
参数设置：
    C = 0.8,
    tol = 0.01,
    maxIter = 30,
    kTup=['lin', 0.8]

9.12
初步完成了SVM.py
着重检查一下predict的while循环

9.16
trainSize = 50*10
testSize = 10*10
0.702 correct
highest
0.7, 0.01, 20, ['lin', 0.8]

 2019-9-30
 0.7846666666666666
[[ 0  0  5  0  0  1 20  0  7  0]
 [ 0  0  0  1  0  0  1  1  5  0]
 [ 0  0  0  3  0  0 10 11 17  0]
 [ 0  0  1  0  0  5  1 10 26  0]
 [ 0  0  1  0  0  0  5 14 17 11]
 [ 3  0  1 10  1  0  3  2 26  3]
 [ 1  1  1  0  0  2  0  0  4  0]
 [ 0  1  2  1  0  0  0  0 12  1]
 [ 1  0  1  5  2  5  1  5  0  1]
 [ 0  0  3  3  3  0  0 32 13  0]]
 国庆快乐

 2019-10-1
 把trTol调成1(取消boost流程)
 0.762
[[ 0  0  9  1  0  1 14  2  5  0]
 [ 0  0  0  1  0  0  1  2  5  0]
 [ 0  1  0  4  1  0 10 13 29  0]
 [ 0  0  0  0  0  6  4 10 22  0]
 [ 1  0  1  0  0  0  6 16 15 13]
 [ 2  0  1  3  2  0  3  5 25  4]
 [ 1  1  1  1  0  2  0  3  7  0]
 [ 0  1  1  3  0  0  0  0  8  5]
 [ 1  0  1  5  0  3  1  5  0  0]
 [ 0  0  1  1  3  2  0 53  9  0]]

 0.7666666666666667
[[ 0  0  6  2  0  0 16  1  7  0]
 [ 0  0  0  1  0  0  1  4  6  0]
 [ 0  0  0  3  0  0 10  9 21  0]
 [ 0  0  0  0  0  6  2 10 25  0]
 [ 0  0  0  0  0  0 10 13 16  7]
 [ 2  0  0  9  1  0  5  4 32  4]
 [ 1  0  3  0  0  0  0  1  6  0]
 [ 0  1  3  1  0  0  1  0 11  0]
 [ 1  0  1  3  0  1  1  5  0  1]
 [ 0  0  0  4  2  0  0 42 28  0]]

 0.7506666666666667
[[ 0  0  5  1  0  0 13  0  8  0]
 [ 0  0  0  1  0  0  1  2  4  0]
 [ 0  0  0  4  0  0 10  9 22  0]
 [ 0  0  1  0  0  2  2  9 29  0]
 [ 0  0  0  1  0  0  7 15 15  2]
 [ 3  0  1  6  1  0  6  5 37  0]
 [ 1  1  1  0  0  1  0  0  6  0]
 [ 0  1  4  0  0  0  1  0  6  0]
 [ 0  0  0  5  0  6  3  2  0  1]
 [ 0  0  0  2  4  1  0 72 34  0]]

 error matrix的特征很明显

 rbf：
 2 0.2866666666666667
 5 0.5933333333333334

 4 0.62

 4.5 0.6

 3.5 0.6333333333333333

 3 0.66
